INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1 --encoder-num-layers 1 --decoder-num-layers 1
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 1, 'decoder_num_layers': 1, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.519 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 31.95 | clip 0.9982
INFO: Epoch 000: valid_loss 4.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 125
INFO: Epoch 001: loss 3.807 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 37.24 | clip 1
INFO: Epoch 001: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 115
INFO: Epoch 002: loss 3.516 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.35 | clip 1
INFO: Epoch 002: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.1
INFO: Epoch 003: loss 3.33 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.93 | clip 1
INFO: Epoch 003: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.5
INFO: Epoch 004: loss 3.189 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.84 | clip 1
INFO: Epoch 004: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.5
INFO: Epoch 005: loss 3.064 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.58 | clip 1
INFO: Epoch 005: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.4
INFO: Epoch 006: loss 2.969 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.87 | clip 1
INFO: Epoch 006: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.2
INFO: Epoch 007: loss 2.871 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.77 | clip 1
INFO: Epoch 007: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.7
INFO: Epoch 008: loss 2.785 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.07 | clip 1
INFO: Epoch 008: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30
INFO: Epoch 009: loss 2.718 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.74 | clip 1
INFO: Epoch 009: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5
INFO: Epoch 010: loss 2.655 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.79 | clip 1
INFO: Epoch 010: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28
INFO: Epoch 011: loss 2.593 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.23 | clip 0.9999
INFO: Epoch 011: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6
INFO: Epoch 012: loss 2.541 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.56 | clip 1
INFO: Epoch 012: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25
INFO: Epoch 013: loss 2.499 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.98 | clip 0.9998
INFO: Epoch 013: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3
INFO: Epoch 014: loss 2.45 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.23 | clip 0.9999
INFO: Epoch 014: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4
INFO: Epoch 015: loss 2.411 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.21 | clip 0.9999
INFO: Epoch 015: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 016: loss 2.377 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.65 | clip 0.9996
INFO: Epoch 016: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8
INFO: Epoch 017: loss 2.339 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.42 | clip 0.9997
INFO: Epoch 017: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4
INFO: Epoch 018: loss 2.307 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.36 | clip 0.9997
INFO: Epoch 018: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20
INFO: Epoch 019: loss 2.279 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.57 | clip 0.9995
INFO: Epoch 019: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5
INFO: Epoch 020: loss 2.259 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.03 | clip 0.9992
INFO: Epoch 020: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 021: loss 2.23 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.63 | clip 0.9994
INFO: Epoch 021: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9
INFO: Epoch 022: loss 2.211 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.42 | clip 0.9994
INFO: Epoch 022: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5
INFO: Epoch 023: loss 2.19 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.21 | clip 0.9998
INFO: Epoch 023: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6
INFO: Epoch 024: loss 2.171 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.15 | clip 0.9995
INFO: Epoch 024: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 025: loss 2.144 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.54 | clip 0.9989
INFO: Epoch 025: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1
INFO: Epoch 026: loss 2.125 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.8 | clip 0.9985
INFO: Epoch 026: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 027: loss 2.109 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.34 | clip 0.999
INFO: Epoch 027: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4
INFO: Epoch 028: loss 2.1 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.36 | clip 0.9989
INFO: Epoch 028: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8
INFO: Epoch 029: loss 2.076 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.19 | clip 0.9993
INFO: Epoch 029: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3
INFO: Epoch 030: loss 2.063 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.45 | clip 0.999
INFO: Epoch 030: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5
INFO: Epoch 031: loss 2.042 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.93 | clip 0.9991
INFO: Epoch 031: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1
INFO: Epoch 032: loss 2.031 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.25 | clip 0.9988
INFO: Epoch 032: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6
INFO: Epoch 033: loss 2.022 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.61 | clip 0.9982
INFO: Epoch 033: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14
INFO: Epoch 034: loss 2.012 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.21 | clip 0.9986
INFO: Epoch 034: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 035: loss 1.992 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.14 | clip 0.9985
INFO: Epoch 035: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 036: loss 1.978 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.39 | clip 0.9982
INFO: Epoch 036: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8
INFO: Epoch 037: loss 1.969 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.95 | clip 0.9981
INFO: Epoch 037: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7
INFO: Epoch 038: loss 1.952 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.36 | clip 0.9978
INFO: Epoch 038: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5
INFO: Epoch 039: loss 1.949 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.2 | clip 0.998
INFO: Epoch 039: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4
INFO: Epoch 040: loss 1.94 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.13 | clip 0.9983
INFO: Epoch 040: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 041: loss 1.931 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.69 | clip 0.9978
INFO: Epoch 041: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 042: loss 1.917 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.24 | clip 0.9982
INFO: Epoch 042: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: Epoch 043: loss 1.906 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.25 | clip 0.9975
INFO: Epoch 043: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 044: loss 1.899 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.92 | clip 0.998
INFO: Epoch 044: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 045: loss 1.893 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.43 | clip 0.9979
INFO: Epoch 045: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 046: loss 1.883 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.21 | clip 0.9972
INFO: Epoch 046: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7
INFO: Epoch 047: loss 1.876 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.07 | clip 0.9982
INFO: Epoch 047: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8
INFO: Epoch 048: loss 1.861 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.11 | clip 0.9964
INFO: Epoch 048: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6
INFO: Epoch 049: loss 1.859 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.36 | clip 0.9965
INFO: Epoch 049: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 050: loss 1.843 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.88 | clip 0.9962
INFO: Epoch 050: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4
INFO: Epoch 051: loss 1.84 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.49 | clip 0.9966
INFO: Epoch 051: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1
INFO: Epoch 052: loss 1.831 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.66 | clip 0.997
INFO: Epoch 052: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 053: loss 1.818 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.15 | clip 0.9966
INFO: Epoch 053: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 054: loss 1.818 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.89 | clip 0.9971
INFO: Epoch 054: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2 --encoder-num-layers 2 --decoder-num-layers 2
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1540000 parameters
INFO: Epoch 000: loss 4.546 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 28.75 | clip 0.9966
INFO: Epoch 000: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150
INFO: Epoch 001: loss 3.841 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 31.7 | clip 1
INFO: Epoch 001: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 132
INFO: Epoch 002: loss 3.566 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 35.15 | clip 1
INFO: Epoch 002: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.6
INFO: Epoch 003: loss 3.379 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 37.33 | clip 1
INFO: Epoch 003: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61
INFO: Epoch 004: loss 3.239 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.52 | clip 1
INFO: Epoch 004: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.1
INFO: Epoch 005: loss 3.124 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.03 | clip 1
INFO: Epoch 005: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.5
INFO: Epoch 006: loss 3.027 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40 | clip 1
INFO: Epoch 006: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.7
INFO: Epoch 007: loss 2.941 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.11 | clip 0.9999
INFO: Epoch 007: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1
INFO: Epoch 008: loss 2.863 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.62 | clip 1
INFO: Epoch 008: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3
INFO: Epoch 009: loss 2.803 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.05 | clip 0.9999
INFO: Epoch 009: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8
INFO: Epoch 010: loss 2.739 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.21 | clip 1
INFO: Epoch 010: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8
INFO: Epoch 011: loss 2.679 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.31 | clip 1
INFO: Epoch 011: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4
INFO: Epoch 012: loss 2.628 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.16 | clip 1
INFO: Epoch 012: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 013: loss 2.578 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.67 | clip 0.9999
INFO: Epoch 013: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8
INFO: Epoch 014: loss 2.539 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.56 | clip 1
INFO: Epoch 014: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9
INFO: Epoch 015: loss 2.495 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.25 | clip 0.9999
INFO: Epoch 015: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4
INFO: Epoch 016: loss 2.46 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.23 | clip 0.9997
INFO: Epoch 016: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1
INFO: Epoch 017: loss 2.423 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.55 | clip 0.9998
INFO: Epoch 017: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 018: loss 2.39 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.34 | clip 0.9997
INFO: Epoch 018: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5
INFO: Epoch 019: loss 2.357 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.4 | clip 0.9998
INFO: Epoch 019: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 020: loss 2.329 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.13 | clip 0.9997
INFO: Epoch 020: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6
INFO: Epoch 021: loss 2.303 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.02 | clip 0.9998
INFO: Epoch 021: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 022: loss 2.283 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.06 | clip 0.9993
INFO: Epoch 022: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16
INFO: Epoch 023: loss 2.256 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.08 | clip 0.9998
INFO: Epoch 023: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1
INFO: Epoch 024: loss 2.242 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41 | clip 0.9998
INFO: Epoch 024: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3
INFO: Epoch 025: loss 2.216 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.82 | clip 0.9996
INFO: Epoch 025: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9
INFO: Epoch 026: loss 2.198 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.96 | clip 0.9995
INFO: Epoch 026: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2
INFO: Epoch 027: loss 2.179 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.71 | clip 0.9995
INFO: Epoch 027: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14
INFO: Epoch 028: loss 2.155 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.71 | clip 0.9995
INFO: Epoch 028: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 029: loss 2.14 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.72 | clip 0.9991
INFO: Epoch 029: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4
INFO: Epoch 030: loss 2.122 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.95 | clip 0.9993
INFO: Epoch 030: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 031: loss 2.107 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.85 | clip 0.9987
INFO: Epoch 031: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: Epoch 032: loss 2.091 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.83 | clip 0.999
INFO: Epoch 032: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9
INFO: Epoch 033: loss 2.079 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.86 | clip 0.9992
INFO: Epoch 033: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 034: loss 2.058 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.73 | clip 0.9992
INFO: Epoch 034: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 035: loss 2.05 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.77 | clip 0.9985
INFO: Epoch 035: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: Epoch 036: loss 2.032 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.67 | clip 0.9996
INFO: Epoch 036: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 037: loss 2.01 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.4 | clip 0.999
INFO: Epoch 037: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8
INFO: Epoch 038: loss 2.01 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.05 | clip 0.9992
INFO: Epoch 038: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 039: loss 1.992 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.66 | clip 0.9981
INFO: Epoch 039: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4
INFO: Epoch 040: loss 1.977 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.19 | clip 0.9981
INFO: Epoch 040: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4
INFO: Epoch 041: loss 1.965 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.79 | clip 0.9981
INFO: Epoch 041: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2
INFO: Epoch 042: loss 1.959 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.85 | clip 0.9986
INFO: Epoch 042: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1
INFO: Epoch 043: loss 1.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.64 | clip 0.9983
INFO: Epoch 043: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 044: loss 1.94 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.21 | clip 0.9984
INFO: Epoch 044: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7
INFO: Epoch 045: loss 1.926 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.97 | clip 0.9972
INFO: Epoch 045: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8
INFO: Epoch 046: loss 1.917 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.92 | clip 0.9974
INFO: Epoch 046: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11
INFO: Epoch 047: loss 1.904 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.24 | clip 0.998
INFO: Epoch 047: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5
INFO: Epoch 048: loss 1.899 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.87 | clip 0.9982
INFO: Epoch 048: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7
INFO: Epoch 049: loss 1.889 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.94 | clip 0.9983
INFO: Epoch 049: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4
INFO: Epoch 050: loss 1.874 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.65 | clip 0.9973
INFO: Epoch 050: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5
INFO: Epoch 051: loss 1.871 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.17 | clip 0.9969
INFO: Epoch 051: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5
INFO: Epoch 052: loss 1.861 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.99 | clip 0.9973
INFO: Epoch 052: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3 --encoder-num-layers 3 --decoder-num-layers 3
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1771424 parameters
INFO: Epoch 000: loss 4.655 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 29.54 | clip 0.9968
INFO: Epoch 000: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 157
INFO: Epoch 001: loss 4.001 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 28.59 | clip 1
INFO: Epoch 001: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116
INFO: Epoch 002: loss 3.723 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 31.9 | clip 1
INFO: Epoch 002: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.9
INFO: Epoch 003: loss 3.546 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 33.57 | clip 1
INFO: Epoch 003: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.3
INFO: Epoch 004: loss 3.408 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 35.38 | clip 1
INFO: Epoch 004: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.1
INFO: Epoch 005: loss 3.282 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 36.21 | clip 1
INFO: Epoch 005: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6
INFO: Epoch 006: loss 3.182 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 36.83 | clip 1
INFO: Epoch 006: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40
INFO: Epoch 007: loss 3.094 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 37.59 | clip 1
INFO: Epoch 007: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.2
INFO: Epoch 008: loss 3.02 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 37.79 | clip 1
INFO: Epoch 008: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.4
INFO: Epoch 009: loss 2.945 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.35 | clip 1
INFO: Epoch 009: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.9
INFO: Epoch 010: loss 2.887 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39 | clip 1
INFO: Epoch 010: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5
INFO: Epoch 011: loss 2.83 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.92 | clip 1
INFO: Epoch 011: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1
INFO: Epoch 012: loss 2.775 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.15 | clip 1
INFO: Epoch 012: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6
INFO: Epoch 013: loss 2.725 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.42 | clip 0.9999
INFO: Epoch 013: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3
INFO: Epoch 014: loss 2.68 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.34 | clip 1
INFO: Epoch 014: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1
INFO: Epoch 015: loss 2.639 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.4 | clip 1
INFO: Epoch 015: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6
INFO: Epoch 016: loss 2.595 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 1
INFO: Epoch 016: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9
INFO: Epoch 017: loss 2.551 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.23 | clip 1
INFO: Epoch 017: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20
INFO: Epoch 018: loss 2.516 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.54 | clip 1
INFO: Epoch 018: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1
INFO: Epoch 019: loss 2.483 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.43 | clip 1
INFO: Epoch 019: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7
INFO: Epoch 020: loss 2.45 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.37 | clip 0.9999
INFO: Epoch 020: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6
INFO: Epoch 021: loss 2.422 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.3 | clip 1
INFO: Epoch 021: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4
INFO: Epoch 022: loss 2.392 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.32 | clip 0.9995
INFO: Epoch 022: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1
INFO: Epoch 023: loss 2.375 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.32 | clip 0.9998
INFO: Epoch 023: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3
INFO: Epoch 024: loss 2.348 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.35 | clip 0.9997
INFO: Epoch 024: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 025: loss 2.327 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 0.9998
INFO: Epoch 025: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4
INFO: Epoch 026: loss 2.307 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.3 | clip 0.9997
INFO: Epoch 026: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17
INFO: Epoch 027: loss 2.286 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.07 | clip 0.9998
INFO: Epoch 027: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1
INFO: Epoch 028: loss 2.254 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.25 | clip 0.9997
INFO: Epoch 028: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 029: loss 2.23 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.5 | clip 0.9998
INFO: Epoch 029: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 030: loss 2.218 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.57 | clip 0.9996
INFO: Epoch 030: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5
INFO: Epoch 031: loss 2.202 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.67 | clip 0.9995
INFO: Epoch 031: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7
INFO: Epoch 032: loss 2.182 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.43 | clip 1
INFO: Epoch 032: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7
INFO: Epoch 033: loss 2.162 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.65 | clip 0.9996
INFO: Epoch 033: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7
INFO: Epoch 034: loss 2.145 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 0.9993
INFO: Epoch 034: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9
INFO: Epoch 035: loss 2.132 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.63 | clip 0.999
INFO: Epoch 035: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5
INFO: Epoch 036: loss 2.116 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.9 | clip 0.9993
INFO: Epoch 036: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8
INFO: Epoch 037: loss 2.1 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.41 | clip 0.999
INFO: Epoch 037: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9
INFO: Epoch 038: loss 2.089 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.37 | clip 0.9986
INFO: Epoch 038: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1
INFO: Epoch 039: loss 2.067 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.49 | clip 0.9987
INFO: Epoch 039: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 040: loss 2.059 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.73 | clip 0.9987
INFO: Epoch 040: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 041: loss 2.046 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.58 | clip 0.9993
INFO: Epoch 041: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 042: loss 2.032 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.77 | clip 0.999
INFO: Epoch 042: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3
INFO: Epoch 043: loss 2.02 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.47 | clip 0.9989
INFO: Epoch 043: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2
INFO: Epoch 044: loss 2.009 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.68 | clip 0.9991
INFO: Epoch 044: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13
INFO: Epoch 045: loss 1.995 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.67 | clip 0.9986
INFO: Epoch 045: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13
INFO: Epoch 046: loss 1.984 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 0.9989
INFO: Epoch 046: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 047: loss 1.979 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.82 | clip 0.9988
INFO: Epoch 047: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4
INFO: Epoch 048: loss 1.96 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.55 | clip 0.9987
INFO: Epoch 048: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: Epoch 049: loss 1.951 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.38 | clip 0.9991
INFO: Epoch 049: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8
INFO: Epoch 050: loss 1.941 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 0.9984
INFO: Epoch 050: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: No validation set improvements observed for 3 epochs. Early stop!
train done!
[2023-11-01 21:52:15] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1.en.txt
[2023-11-01 21:52:15] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 1, 'decoder_num_layers': 1, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1.en.txt', 'max_len': 128}
[2023-11-01 21:52:15] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 21:52:15] Loaded a target dictionary (en) with 4000 words
[2023-11-01 21:52:15] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_1/checkpoint_last.pt
[2023-11-01 22:09:49] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2.en.txt
[2023-11-01 22:09:49] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2.en.txt', 'max_len': 128}
[2023-11-01 22:09:49] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 22:09:49] Loaded a target dictionary (en) with 4000 words
[2023-11-01 22:09:49] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_2/checkpoint_last.pt
[2023-11-01 22:32:08] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3.en.txt
[2023-11-01 22:32:08] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3.en.txt', 'max_len': 128}
[2023-11-01 22:32:08] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 22:32:08] Loaded a target dictionary (en) with 4000 words
[2023-11-01 22:32:08] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--model/num_layers_3/checkpoint_last.pt
translate done!
{
 "name": "BLEU",
 "score": 17.1,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "45.1/22.3/12.4/6.8 (BP = 1.000 ratio = 1.309 hyp_len = 5096 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
{
 "name": "BLEU",
 "score": 17.9,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "46.6/23.1/13.1/7.2 (BP = 1.000 ratio = 1.251 hyp_len = 4867 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
{
 "name": "BLEU",
 "score": 13.5,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "41.3/18.6/9.4/4.6 (BP = 1.000 ratio = 1.376 hyp_len = 5357 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
