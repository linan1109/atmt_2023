INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --lr 0.0001 --batch-size 1 --max-epoch 10000 --patience 3 --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001 --restore-file checkpoint-last.pt --save-interval 1 --encoder-embed-dim 64 --encoder-hidden-size 64 --encoder-num-layers 1 --encoder-bidirectional True --encoder-dropout-in 0.25 --encoder-dropout-out 0.25 --decoder-embed-dim 64 --decoder-hidden-size 128 --decoder-num-layers 1 --decoder-dropout-in 0.25 --decoder-dropout-out 0.25
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0001, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.876 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 32.63 | clip 0.9962
INFO: Epoch 000: valid_loss 5.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 172
INFO: Epoch 001: loss 4.344 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 33.49 | clip 1
INFO: Epoch 001: valid_loss 4.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135
INFO: Epoch 002: loss 4.126 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 35.78 | clip 1
INFO: Epoch 002: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110
INFO: Epoch 003: loss 3.964 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 38.31 | clip 1
INFO: Epoch 003: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.5
INFO: Epoch 004: loss 3.839 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 42.02 | clip 1
INFO: Epoch 004: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.1
INFO: Epoch 005: loss 3.728 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 43.09 | clip 1
INFO: Epoch 005: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.4
INFO: Epoch 006: loss 3.632 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 44.57 | clip 1
INFO: Epoch 006: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.4
INFO: Epoch 007: loss 3.546 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 45.73 | clip 1
INFO: Epoch 007: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.6
INFO: Epoch 008: loss 3.457 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 47.76 | clip 1
INFO: Epoch 008: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.5
INFO: Epoch 009: loss 3.382 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 47.93 | clip 1
INFO: Epoch 009: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.2
INFO: Epoch 010: loss 3.316 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 49.25 | clip 1
INFO: Epoch 010: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.4
INFO: Epoch 011: loss 3.254 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 50.3 | clip 1
INFO: Epoch 011: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.5
INFO: Epoch 012: loss 3.202 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 50.95 | clip 1
INFO: Epoch 012: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1
INFO: Epoch 013: loss 3.15 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 51.98 | clip 1
INFO: Epoch 013: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.9
INFO: Epoch 014: loss 3.099 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 52.87 | clip 1
INFO: Epoch 014: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.9
INFO: Epoch 015: loss 3.054 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 53.98 | clip 1
INFO: Epoch 015: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.3
INFO: Epoch 016: loss 3.018 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 54.49 | clip 1
INFO: Epoch 016: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5
INFO: Epoch 017: loss 2.968 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 55.3 | clip 1
INFO: Epoch 017: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.1
INFO: Epoch 018: loss 2.93 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 56.19 | clip 1
INFO: Epoch 018: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31
INFO: Epoch 019: loss 2.886 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 57.08 | clip 1
INFO: Epoch 019: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.6
INFO: Epoch 020: loss 2.851 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 59.26 | clip 1
INFO: Epoch 020: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7
INFO: Epoch 021: loss 2.815 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 58.19 | clip 1
INFO: Epoch 021: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8
INFO: Epoch 022: loss 2.782 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 58.97 | clip 1
INFO: Epoch 022: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.5
INFO: Epoch 023: loss 2.75 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 59.89 | clip 1
INFO: Epoch 023: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9
INFO: Epoch 024: loss 2.718 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 60.04 | clip 1
INFO: Epoch 024: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25
INFO: Epoch 025: loss 2.683 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 60.07 | clip 1
INFO: Epoch 025: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2
INFO: Epoch 026: loss 2.657 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 61.78 | clip 1
INFO: Epoch 026: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8
INFO: Epoch 027: loss 2.635 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 60.82 | clip 1
INFO: Epoch 027: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1
INFO: Epoch 028: loss 2.603 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 60.98 | clip 1
INFO: Epoch 028: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6
INFO: Epoch 029: loss 2.576 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 61.67 | clip 1
INFO: Epoch 029: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5
INFO: Epoch 030: loss 2.552 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 62.04 | clip 1
INFO: Epoch 030: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.2
INFO: Epoch 031: loss 2.535 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 61.92 | clip 1
INFO: Epoch 031: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5
INFO: Epoch 032: loss 2.508 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 62.44 | clip 0.9999
INFO: Epoch 032: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1
INFO: Epoch 033: loss 2.484 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 62.35 | clip 0.9998
INFO: Epoch 033: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9
INFO: Epoch 034: loss 2.462 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.07 | clip 0.9999
INFO: Epoch 034: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8
INFO: Epoch 035: loss 2.448 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.09 | clip 0.9999
INFO: Epoch 035: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7
INFO: Epoch 036: loss 2.429 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.83 | clip 0.9998
INFO: Epoch 036: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3
INFO: Epoch 037: loss 2.412 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.4 | clip 0.9998
INFO: Epoch 037: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5
INFO: Epoch 038: loss 2.392 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.11 | clip 1
INFO: Epoch 038: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9
INFO: Epoch 039: loss 2.37 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.81 | clip 0.9997
INFO: Epoch 039: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3
INFO: Epoch 040: loss 2.357 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.52 | clip 0.9999
INFO: Epoch 040: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4
INFO: Epoch 041: loss 2.339 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.43 | clip 0.9997
INFO: Epoch 041: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2
INFO: Epoch 042: loss 2.323 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.54 | clip 0.9996
INFO: Epoch 042: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 043: loss 2.302 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.17 | clip 0.9997
INFO: Epoch 043: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 044: loss 2.294 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.4 | clip 0.9997
INFO: Epoch 044: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 045: loss 2.281 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 65.02 | clip 0.9996
INFO: Epoch 045: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 046: loss 2.266 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.84 | clip 0.9998
INFO: Epoch 046: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9
INFO: Epoch 047: loss 2.246 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.41 | clip 1
INFO: Epoch 047: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4
INFO: Epoch 048: loss 2.236 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.77 | clip 0.9997
INFO: Epoch 048: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1
INFO: Epoch 049: loss 2.225 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.9 | clip 0.9999
INFO: Epoch 049: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9
INFO: Epoch 050: loss 2.213 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.28 | clip 0.9994
INFO: Epoch 050: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15
INFO: Epoch 051: loss 2.199 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 65.05 | clip 0.9996
INFO: Epoch 051: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6
INFO: Epoch 052: loss 2.189 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.37 | clip 0.9993
INFO: Epoch 052: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5
INFO: Epoch 053: loss 2.181 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.91 | clip 0.9997
INFO: Epoch 053: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8
INFO: Epoch 054: loss 2.168 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 65.19 | clip 0.9994
INFO: Epoch 054: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2
INFO: Epoch 055: loss 2.154 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.73 | clip 0.9993
INFO: Epoch 055: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3
INFO: Epoch 056: loss 2.146 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.43 | clip 0.9996
INFO: Epoch 056: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4
INFO: Epoch 057: loss 2.141 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 65.02 | clip 0.9991
INFO: Epoch 057: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14
INFO: Epoch 058: loss 2.13 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.99 | clip 0.9996
INFO: Epoch 058: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9
INFO: Epoch 059: loss 2.122 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.77 | clip 0.9994
INFO: Epoch 059: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6
INFO: Epoch 060: loss 2.117 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.9 | clip 0.9993
INFO: Epoch 060: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6
INFO: Epoch 061: loss 2.104 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.29 | clip 0.9993
INFO: Epoch 061: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4
INFO: Epoch 062: loss 2.093 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.38 | clip 0.9996
INFO: Epoch 062: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3
INFO: Epoch 063: loss 2.085 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.82 | clip 0.999
INFO: Epoch 063: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4
INFO: Epoch 064: loss 2.072 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.72 | clip 0.9987
INFO: Epoch 064: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 065: loss 2.063 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.15 | clip 0.9992
INFO: Epoch 065: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 066: loss 2.068 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.6 | clip 0.9995
INFO: Epoch 066: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7
INFO: Epoch 067: loss 2.056 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.83 | clip 0.9993
INFO: Epoch 067: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9
INFO: Epoch 068: loss 2.046 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.71 | clip 0.9993
INFO: Epoch 068: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8
INFO: Epoch 069: loss 2.037 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.89 | clip 0.999
INFO: Epoch 069: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5
INFO: Epoch 070: loss 2.027 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.34 | clip 0.9989
INFO: Epoch 070: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: Epoch 071: loss 2.024 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.41 | clip 0.9991
INFO: Epoch 071: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 072: loss 2.014 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 63.79 | clip 0.9989
INFO: Epoch 072: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5
INFO: Epoch 073: loss 2.005 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.52 | clip 0.999
INFO: Epoch 073: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 074: loss 2.001 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.37 | clip 0.9982
INFO: Epoch 074: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 075: loss 1.993 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.03 | clip 0.9991
INFO: Epoch 075: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12
INFO: Epoch 076: loss 1.983 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.07 | clip 0.9988
INFO: Epoch 076: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 077: loss 1.981 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.21 | clip 0.9983
INFO: Epoch 077: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 078: loss 1.973 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.24 | clip 0.9981
INFO: Epoch 078: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 079: loss 1.972 | lr 0.0001 | num_tokens 9.1 | batch_size 1 | grad_norm 64.66 | clip 0.9989
INFO: Epoch 079: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --lr 0.0005 --batch-size 1 --max-epoch 10000 --patience 3 --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005 --restore-file checkpoint-last.pt --save-interval 1 --encoder-embed-dim 64 --encoder-hidden-size 64 --encoder-num-layers 1 --encoder-bidirectional True --encoder-dropout-in 0.25 --encoder-dropout-out 0.25 --decoder-embed-dim 64 --decoder-hidden-size 128 --decoder-num-layers 1 --decoder-dropout-in 0.25 --decoder-dropout-out 0.25
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.349 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 33.47 | clip 0.9987
INFO: Epoch 000: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.3
INFO: Epoch 001: loss 3.599 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 38.14 | clip 1
INFO: Epoch 001: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.8
INFO: Epoch 002: loss 3.307 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 40.91 | clip 1
INFO: Epoch 002: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.1
INFO: Epoch 003: loss 3.115 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 42.71 | clip 1
INFO: Epoch 003: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.4
INFO: Epoch 004: loss 2.976 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 44.09 | clip 1
INFO: Epoch 004: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6
INFO: Epoch 005: loss 2.85 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.06 | clip 1
INFO: Epoch 005: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.6
INFO: Epoch 006: loss 2.752 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.64 | clip 1
INFO: Epoch 006: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.4
INFO: Epoch 007: loss 2.665 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.17 | clip 0.9999
INFO: Epoch 007: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4
INFO: Epoch 008: loss 2.592 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.29 | clip 0.9999
INFO: Epoch 008: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3
INFO: Epoch 009: loss 2.533 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.46 | clip 0.9998
INFO: Epoch 009: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4
INFO: Epoch 010: loss 2.481 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.68 | clip 0.9994
INFO: Epoch 010: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3
INFO: Epoch 011: loss 2.432 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.9 | clip 0.9998
INFO: Epoch 011: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1
INFO: Epoch 012: loss 2.385 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.52 | clip 0.9996
INFO: Epoch 012: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1
INFO: Epoch 013: loss 2.354 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.64 | clip 0.9995
INFO: Epoch 013: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19
INFO: Epoch 014: loss 2.316 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.29 | clip 0.9998
INFO: Epoch 014: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8
INFO: Epoch 015: loss 2.295 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.94 | clip 0.9993
INFO: Epoch 015: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17
INFO: Epoch 016: loss 2.264 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.51 | clip 0.9996
INFO: Epoch 016: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17
INFO: Epoch 017: loss 2.237 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.05 | clip 0.9994
INFO: Epoch 017: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3
INFO: Epoch 018: loss 2.209 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.15 | clip 0.9995
INFO: Epoch 018: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 019: loss 2.193 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.91 | clip 0.9992
INFO: Epoch 019: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 020: loss 2.166 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.88 | clip 0.999
INFO: Epoch 020: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14
INFO: Epoch 021: loss 2.139 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.64 | clip 0.999
INFO: Epoch 021: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6
INFO: Epoch 022: loss 2.126 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.6 | clip 0.9994
INFO: Epoch 022: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4
INFO: Epoch 023: loss 2.105 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.01 | clip 0.9995
INFO: Epoch 023: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6
INFO: Epoch 024: loss 2.096 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.08 | clip 0.9991
INFO: Epoch 024: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6
INFO: Epoch 025: loss 2.076 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.01 | clip 0.9986
INFO: Epoch 025: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 026: loss 2.063 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.3 | clip 0.9989
INFO: Epoch 026: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13
INFO: Epoch 027: loss 2.051 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 46.01 | clip 0.9984
INFO: Epoch 027: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1
INFO: Epoch 028: loss 2.035 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.83 | clip 0.9986
INFO: Epoch 028: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 029: loss 2.016 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.86 | clip 0.9982
INFO: Epoch 029: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 030: loss 2.008 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.98 | clip 0.9988
INFO: Epoch 030: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: Epoch 031: loss 1.999 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.67 | clip 0.9984
INFO: Epoch 031: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 032: loss 1.991 | lr 0.0005 | num_tokens 9.1 | batch_size 1 | grad_norm 45.74 | clip 0.9981
INFO: Epoch 032: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --lr 0.001 --batch-size 1 --max-epoch 10000 --patience 3 --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001 --restore-file checkpoint-last.pt --save-interval 1 --encoder-embed-dim 64 --encoder-hidden-size 64 --encoder-num-layers 1 --encoder-bidirectional True --encoder-dropout-in 0.25 --encoder-dropout-out 0.25 --decoder-embed-dim 64 --decoder-hidden-size 128 --decoder-num-layers 1 --decoder-dropout-in 0.25 --decoder-dropout-out 0.25
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.001, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.098 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 35.14 | clip 0.9988
INFO: Epoch 000: valid_loss 4.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 124
INFO: Epoch 001: loss 3.402 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 37.3 | clip 1
INFO: Epoch 001: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.3
INFO: Epoch 002: loss 3.149 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 38.47 | clip 0.9999
INFO: Epoch 002: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.2
INFO: Epoch 003: loss 2.966 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 38.92 | clip 0.9998
INFO: Epoch 003: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.2
INFO: Epoch 004: loss 2.839 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 38.93 | clip 0.9999
INFO: Epoch 004: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2
INFO: Epoch 005: loss 2.742 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.51 | clip 0.9997
INFO: Epoch 005: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6
INFO: Epoch 006: loss 2.666 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.25 | clip 0.9997
INFO: Epoch 006: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2
INFO: Epoch 007: loss 2.601 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.04 | clip 0.9997
INFO: Epoch 007: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.5
INFO: Epoch 008: loss 2.542 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.38 | clip 0.9996
INFO: Epoch 008: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5
INFO: Epoch 009: loss 2.504 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.75 | clip 0.9995
INFO: Epoch 009: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5
INFO: Epoch 010: loss 2.47 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.45 | clip 0.9993
INFO: Epoch 010: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1
INFO: Epoch 011: loss 2.432 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.46 | clip 0.9996
INFO: Epoch 011: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 012: loss 2.394 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.52 | clip 0.9998
INFO: Epoch 012: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16
INFO: Epoch 013: loss 2.376 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.4 | clip 0.9992
INFO: Epoch 013: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7
INFO: Epoch 014: loss 2.349 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.63 | clip 0.9996
INFO: Epoch 014: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7
INFO: Epoch 015: loss 2.324 | lr 0.001 | num_tokens 9.1 | batch_size 1 | grad_norm 39.75 | clip 0.9992
INFO: Epoch 015: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --lr 0.005 --batch-size 1 --max-epoch 10000 --patience 3 --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005 --restore-file checkpoint-last.pt --save-interval 1 --encoder-embed-dim 64 --encoder-hidden-size 64 --encoder-num-layers 1 --encoder-bidirectional True --encoder-dropout-in 0.25 --encoder-dropout-out 0.25 --decoder-embed-dim 64 --decoder-hidden-size 128 --decoder-num-layers 1 --decoder-dropout-in 0.25 --decoder-dropout-out 0.25
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.005, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.502 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 34.9 | clip 0.9998
INFO: Epoch 000: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.5
INFO: Epoch 001: loss 3.971 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 27.21 | clip 0.9999
INFO: Epoch 001: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.1
INFO: Epoch 002: loss 3.82 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 26.1 | clip 0.9999
INFO: Epoch 002: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.6
INFO: Epoch 003: loss 3.733 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 25.34 | clip 0.9998
INFO: Epoch 003: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.2
INFO: Epoch 004: loss 3.678 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 24.78 | clip 0.9999
INFO: Epoch 004: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.4
INFO: Epoch 005: loss 3.664 | lr 0.005 | num_tokens 9.1 | batch_size 1 | grad_norm 24.38 | clip 0.9998
INFO: Epoch 005: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --lr 0.01 --batch-size 1 --max-epoch 10000 --patience 3 --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01 --restore-file checkpoint-last.pt --save-interval 1 --encoder-embed-dim 64 --encoder-hidden-size 64 --encoder-num-layers 1 --encoder-bidirectional True --encoder-dropout-in 0.25 --encoder-dropout-out 0.25 --decoder-embed-dim 64 --decoder-hidden-size 128 --decoder-num-layers 1 --decoder-dropout-in 0.25 --decoder-dropout-out 0.25
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.01, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 7.86 | lr 0.01 | num_tokens 9.1 | batch_size 1 | grad_norm 4.177e+10 | clip 0.9999
INFO: Epoch 000: valid_loss 7.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.81e+03
INFO: Epoch 001: loss 9.119 | lr 0.01 | num_tokens 9.1 | batch_size 1 | grad_norm 4.171e+04 | clip 1
INFO: Epoch 001: valid_loss 7.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.45e+03
INFO: Epoch 002: loss 9.488 | lr 0.01 | num_tokens 9.1 | batch_size 1 | grad_norm inf | clip 1
INFO: Epoch 002: valid_loss 9.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.07e+04
INFO: Epoch 003: loss 10.16 | lr 0.01 | num_tokens 9.1 | batch_size 1 | grad_norm 1.489e+15 | clip 0.9998
INFO: Epoch 003: valid_loss 7.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.91e+03
INFO: Epoch 004: loss 10.4 | lr 0.01 | num_tokens 9.1 | batch_size 1 | grad_norm 7.456e+08 | clip 1
INFO: Epoch 004: valid_loss 8.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.58e+03
INFO: No validation set improvements observed for 3 epochs. Early stop!
train done!
[2023-10-31 17:31:50] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001.en.txt
[2023-10-31 17:31:50] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0001, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001.en.txt', 'max_len': 128}
[2023-10-31 17:31:50] Loaded a source dictionary (fr) with 4000 words
[2023-10-31 17:31:50] Loaded a target dictionary (en) with 4000 words
[2023-10-31 17:31:50] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0001/checkpoint_last.pt
[2023-10-31 17:48:41] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005.en.txt
[2023-10-31 17:48:41] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005.en.txt', 'max_len': 128}
[2023-10-31 17:48:41] Loaded a source dictionary (fr) with 4000 words
[2023-10-31 17:48:41] Loaded a target dictionary (en) with 4000 words
[2023-10-31 17:48:41] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.0005/checkpoint_last.pt
[2023-10-31 18:05:12] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001.en.txt
[2023-10-31 18:05:12] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.001, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001.en.txt', 'max_len': 128}
[2023-10-31 18:05:12] Loaded a source dictionary (fr) with 4000 words
[2023-10-31 18:05:12] Loaded a target dictionary (en) with 4000 words
[2023-10-31 18:05:12] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.001/checkpoint_last.pt
[2023-10-31 18:21:28] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005.en.txt
[2023-10-31 18:21:28] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.005, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005.en.txt', 'max_len': 128}
[2023-10-31 18:21:28] Loaded a source dictionary (fr) with 4000 words
[2023-10-31 18:21:28] Loaded a target dictionary (en) with 4000 words
[2023-10-31 18:21:28] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.005/checkpoint_last.pt
[2023-10-31 18:38:01] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01.en.txt
[2023-10-31 18:38:01] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.01, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01', 'restore_file': 'checkpoint-last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'encoder_embed_path': None, 'decoder_embed_path': None, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01.en.txt', 'max_len': 128}
[2023-10-31 18:38:01] Loaded a source dictionary (fr) with 4000 words
[2023-10-31 18:38:01] Loaded a target dictionary (en) with 4000 words
[2023-10-31 18:38:01] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--lr/lr_0.01/checkpoint_last.pt
translate done!
---------------------------------
lr_0.0001
{
 "name": "BLEU",
 "score": 15.8,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "45.2/20.9/11.0/5.9 (BP = 1.000 ratio = 1.267 hyp_len = 4931 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
---------------------------------
lr_0.0005
{
 "name": "BLEU",
 "score": 14.7,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "42.5/20.1/10.4/5.2 (BP = 1.000 ratio = 1.330 hyp_len = 5177 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
---------------------------------
lr_0.001
{
 "name": "BLEU",
 "score": 9.3,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "33.0/13.5/6.3/2.7 (BP = 1.000 ratio = 1.478 hyp_len = 5754 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
---------------------------------
lr_0.005
{
 "name": "BLEU",
 "score": 0.2,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "14.9/2.2/0.5/0.1 (BP = 0.201 ratio = 0.384 hyp_len = 1493 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
---------------------------------
lr_0.01
{
 "name": "BLEU",
 "score": 0.1,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "4.3/0.1/0.0/0.0 (BP = 1.000 ratio = 1.728 hyp_len = 6724 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
