INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0 --encoder-dropout-in 0 --encoder-dropout-out 0 --decoder-dropout-in 0 --decoder-dropout-out 0
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.0, 'encoder_dropout_out': 0.0, 'decoder_dropout_in': 0.0, 'decoder_dropout_out': 0.0, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.376 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 29.05 | clip 0.9979
INFO: Epoch 000: valid_loss 4.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 139
INFO: Epoch 001: loss 3.54 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 34.29 | clip 1
INFO: Epoch 001: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114
INFO: Epoch 002: loss 3.129 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.61 | clip 1
INFO: Epoch 002: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96
INFO: Epoch 003: loss 2.817 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.43 | clip 1
INFO: Epoch 003: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.9
INFO: Epoch 004: loss 2.548 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.12 | clip 0.9999
INFO: Epoch 004: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.5
INFO: Epoch 005: loss 2.315 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.52 | clip 0.9999
INFO: Epoch 005: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7
INFO: Epoch 006: loss 2.116 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 55.71 | clip 0.9998
INFO: Epoch 006: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.9
INFO: Epoch 007: loss 1.943 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 57.43 | clip 0.9991
INFO: Epoch 007: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8
INFO: Epoch 008: loss 1.791 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 60.28 | clip 0.9979
INFO: Epoch 008: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.6
INFO: Epoch 009: loss 1.659 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 63.85 | clip 0.9948
INFO: Epoch 009: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.4
INFO: Epoch 010: loss 1.543 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 66.06 | clip 0.9934
INFO: Epoch 010: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.2
INFO: Epoch 011: loss 1.442 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 68.39 | clip 0.9887
INFO: Epoch 011: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35
INFO: Epoch 012: loss 1.35 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 70.09 | clip 0.9825
INFO: Epoch 012: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8
INFO: Epoch 013: loss 1.267 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 75.47 | clip 0.9761
INFO: Epoch 013: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.3
INFO: Epoch 014: loss 1.194 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 74.98 | clip 0.9703
INFO: Epoch 014: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.6
INFO: Epoch 015: loss 1.128 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 76.58 | clip 0.9612
INFO: Epoch 015: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.7
INFO: Epoch 016: loss 1.067 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 80.97 | clip 0.9519
INFO: Epoch 016: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3 --encoder-dropout-in 0.3 --encoder-dropout-out 0.3 --decoder-dropout-in 0.3 --decoder-dropout-out 0.3
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.3, 'encoder_dropout_out': 0.3, 'decoder_dropout_in': 0.3, 'decoder_dropout_out': 0.3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.564 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 32.57 | clip 0.9986
INFO: Epoch 000: valid_loss 4.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 129
INFO: Epoch 001: loss 3.921 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.83 | clip 1
INFO: Epoch 001: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 119
INFO: Epoch 002: loss 3.621 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.63 | clip 1
INFO: Epoch 002: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.3
INFO: Epoch 003: loss 3.435 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.63 | clip 1
INFO: Epoch 003: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.2
INFO: Epoch 004: loss 3.307 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.42 | clip 1
INFO: Epoch 004: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.1
INFO: Epoch 005: loss 3.196 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.98 | clip 1
INFO: Epoch 005: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.2
INFO: Epoch 006: loss 3.115 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.39 | clip 1
INFO: Epoch 006: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.6
INFO: Epoch 007: loss 3.031 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.97 | clip 1
INFO: Epoch 007: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1
INFO: Epoch 008: loss 2.955 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.71 | clip 1
INFO: Epoch 008: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8
INFO: Epoch 009: loss 2.882 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.51 | clip 1
INFO: Epoch 009: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35
INFO: Epoch 010: loss 2.832 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.77 | clip 0.9999
INFO: Epoch 010: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8
INFO: Epoch 011: loss 2.78 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.18 | clip 1
INFO: Epoch 011: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1
INFO: Epoch 012: loss 2.724 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.01 | clip 1
INFO: Epoch 012: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.7
INFO: Epoch 013: loss 2.686 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.34 | clip 1
INFO: Epoch 013: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6
INFO: Epoch 014: loss 2.638 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.11 | clip 1
INFO: Epoch 014: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3
INFO: Epoch 015: loss 2.616 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.36 | clip 0.9999
INFO: Epoch 015: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2
INFO: Epoch 016: loss 2.57 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.09 | clip 1
INFO: Epoch 016: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8
INFO: Epoch 017: loss 2.533 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.2 | clip 1
INFO: Epoch 017: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6
INFO: Epoch 018: loss 2.508 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.1 | clip 0.9999
INFO: Epoch 018: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6
INFO: Epoch 019: loss 2.478 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.81 | clip 0.9999
INFO: Epoch 019: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2
INFO: Epoch 020: loss 2.457 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.02 | clip 0.9998
INFO: Epoch 020: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5
INFO: Epoch 021: loss 2.435 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.57 | clip 0.9999
INFO: Epoch 021: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8
INFO: Epoch 022: loss 2.414 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.51 | clip 1
INFO: Epoch 022: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4
INFO: Epoch 023: loss 2.399 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.43 | clip 1
INFO: Epoch 023: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1
INFO: Epoch 024: loss 2.38 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.45 | clip 1
INFO: Epoch 024: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 025: loss 2.36 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.64 | clip 1
INFO: Epoch 025: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8
INFO: Epoch 026: loss 2.337 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.36 | clip 0.9997
INFO: Epoch 026: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2
INFO: Epoch 027: loss 2.328 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.22 | clip 1
INFO: Epoch 027: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7
INFO: Epoch 028: loss 2.313 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.12 | clip 0.9999
INFO: Epoch 028: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4
INFO: Epoch 029: loss 2.298 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.66 | clip 0.9999
INFO: Epoch 029: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5
INFO: Epoch 030: loss 2.278 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.48 | clip 1
INFO: Epoch 030: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 031: loss 2.267 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.25 | clip 0.9997
INFO: Epoch 031: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8
INFO: Epoch 032: loss 2.255 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.54 | clip 0.9994
INFO: Epoch 032: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2
INFO: Epoch 033: loss 2.231 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.97 | clip 0.9997
INFO: Epoch 033: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13
INFO: Epoch 034: loss 2.234 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.86 | clip 0.9994
INFO: Epoch 034: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6
INFO: Epoch 035: loss 2.212 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.31 | clip 0.9997
INFO: Epoch 035: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1
INFO: Epoch 036: loss 2.209 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.38 | clip 0.9995
INFO: Epoch 036: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5
INFO: Epoch 037: loss 2.201 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.07 | clip 0.9995
INFO: Epoch 037: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 038: loss 2.183 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.29 | clip 0.9994
INFO: Epoch 038: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5
INFO: Epoch 039: loss 2.177 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.55 | clip 0.9996
INFO: Epoch 039: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3
INFO: Epoch 040: loss 2.163 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.8 | clip 0.9993
INFO: Epoch 040: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: Epoch 041: loss 2.151 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.15 | clip 0.9996
INFO: Epoch 041: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2
INFO: Epoch 042: loss 2.142 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.44 | clip 0.9991
INFO: Epoch 042: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1
INFO: Epoch 043: loss 2.134 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.34 | clip 0.9993
INFO: Epoch 043: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 044: loss 2.124 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.16 | clip 0.9993
INFO: Epoch 044: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9
INFO: Epoch 045: loss 2.116 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.43 | clip 0.9992
INFO: Epoch 045: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8
INFO: Epoch 046: loss 2.109 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.36 | clip 0.9992
INFO: Epoch 046: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 047: loss 2.105 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.67 | clip 0.9989
INFO: Epoch 047: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 048: loss 2.097 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.55 | clip 0.999
INFO: Epoch 048: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6
INFO: Epoch 049: loss 2.085 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.67 | clip 0.9985
INFO: Epoch 049: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3
INFO: Epoch 050: loss 2.077 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.75 | clip 0.9988
INFO: Epoch 050: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 051: loss 2.069 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.31 | clip 0.9991
INFO: Epoch 051: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: Epoch 052: loss 2.059 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 49.08 | clip 0.9986
INFO: Epoch 052: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5 --encoder-dropout-in 0.5 --encoder-dropout-out 0.5 --decoder-dropout-in 0.5 --decoder-dropout-out 0.5
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.5, 'encoder_dropout_out': 0.5, 'decoder_dropout_in': 0.5, 'decoder_dropout_out': 0.5, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 4.759 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 37.08 | clip 1
INFO: Epoch 000: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135
INFO: Epoch 001: loss 4.277 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.75 | clip 1
INFO: Epoch 001: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.6
INFO: Epoch 002: loss 4.055 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.88 | clip 1
INFO: Epoch 002: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.8
INFO: Epoch 003: loss 3.916 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.52 | clip 1
INFO: Epoch 003: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66
INFO: Epoch 004: loss 3.81 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.03 | clip 1
INFO: Epoch 004: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.7
INFO: Epoch 005: loss 3.734 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.87 | clip 1
INFO: Epoch 005: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.1
INFO: Epoch 006: loss 3.68 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.05 | clip 1
INFO: Epoch 006: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.3
INFO: Epoch 007: loss 3.625 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.78 | clip 1
INFO: Epoch 007: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.9
INFO: Epoch 008: loss 3.578 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.72 | clip 1
INFO: Epoch 008: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.5
INFO: Epoch 009: loss 3.535 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.46 | clip 1
INFO: Epoch 009: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.4
INFO: Epoch 010: loss 3.494 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.83 | clip 1
INFO: Epoch 010: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.4
INFO: Epoch 011: loss 3.455 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.79 | clip 1
INFO: Epoch 011: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3
INFO: Epoch 012: loss 3.424 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.28 | clip 1
INFO: Epoch 012: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7
INFO: Epoch 013: loss 3.391 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.27 | clip 1
INFO: Epoch 013: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.8
INFO: Epoch 014: loss 3.354 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.84 | clip 1
INFO: Epoch 014: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38
INFO: Epoch 015: loss 3.322 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.73 | clip 1
INFO: Epoch 015: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.1
INFO: Epoch 016: loss 3.296 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.26 | clip 1
INFO: Epoch 016: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2
INFO: Epoch 017: loss 3.262 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.87 | clip 1
INFO: Epoch 017: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.8
INFO: Epoch 018: loss 3.231 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.37 | clip 1
INFO: Epoch 018: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9
INFO: Epoch 019: loss 3.206 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.13 | clip 1
INFO: Epoch 019: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1
INFO: Epoch 020: loss 3.19 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.61 | clip 1
INFO: Epoch 020: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.7
INFO: Epoch 021: loss 3.167 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.97 | clip 1
INFO: Epoch 021: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.3
INFO: Epoch 022: loss 3.148 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.34 | clip 1
INFO: Epoch 022: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8
INFO: Epoch 023: loss 3.131 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.55 | clip 1
INFO: Epoch 023: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9
INFO: Epoch 024: loss 3.119 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.07 | clip 1
INFO: Epoch 024: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8
INFO: Epoch 025: loss 3.108 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.28 | clip 1
INFO: Epoch 025: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.2
INFO: Epoch 026: loss 3.086 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.15 | clip 1
INFO: Epoch 026: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4
INFO: Epoch 027: loss 3.082 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.95 | clip 1
INFO: Epoch 027: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.3
INFO: Epoch 028: loss 3.073 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.2 | clip 1
INFO: Epoch 028: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8
INFO: Epoch 029: loss 3.057 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.2 | clip 1
INFO: Epoch 029: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27
INFO: Epoch 030: loss 3.045 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.3 | clip 1
INFO: Epoch 030: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6
INFO: Epoch 031: loss 3.035 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.37 | clip 1
INFO: Epoch 031: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26
INFO: Epoch 032: loss 3.026 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.29 | clip 1
INFO: Epoch 032: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3
INFO: Epoch 033: loss 3.021 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.25 | clip 1
INFO: Epoch 033: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7
INFO: Epoch 034: loss 3.011 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.28 | clip 1
INFO: Epoch 034: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6
INFO: Epoch 035: loss 2.996 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.55 | clip 1
INFO: Epoch 035: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24
INFO: Epoch 036: loss 2.988 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.94 | clip 1
INFO: Epoch 036: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2
INFO: Epoch 037: loss 2.979 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.94 | clip 1
INFO: Epoch 037: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24
INFO: Epoch 038: loss 2.965 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.11 | clip 1
INFO: Epoch 038: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 039: loss 2.967 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.83 | clip 1
INFO: Epoch 039: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6
INFO: Epoch 040: loss 2.948 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.71 | clip 1
INFO: Epoch 040: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1
INFO: Epoch 041: loss 2.948 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.13 | clip 1
INFO: Epoch 041: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8
INFO: Epoch 042: loss 2.934 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.21 | clip 1
INFO: Epoch 042: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 043: loss 2.925 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.89 | clip 1
INFO: Epoch 043: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3
INFO: Epoch 044: loss 2.921 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 43.9 | clip 1
INFO: Epoch 044: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5
INFO: Epoch 045: loss 2.915 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.25 | clip 1
INFO: Epoch 045: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2
INFO: Epoch 046: loss 2.907 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.35 | clip 1
INFO: Epoch 046: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20
INFO: Epoch 047: loss 2.903 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.41 | clip 1
INFO: Epoch 047: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3
INFO: Epoch 048: loss 2.9 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.67 | clip 1
INFO: Epoch 048: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7
INFO: Epoch 049: loss 2.894 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.73 | clip 1
INFO: Epoch 049: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8
INFO: Epoch 050: loss 2.887 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.52 | clip 1
INFO: Epoch 050: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19
INFO: Epoch 051: loss 2.88 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.79 | clip 1
INFO: Epoch 051: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3
INFO: Epoch 052: loss 2.873 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.9 | clip 1
INFO: Epoch 052: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9
INFO: Epoch 053: loss 2.867 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.08 | clip 1
INFO: Epoch 053: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9
INFO: Epoch 054: loss 2.865 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.04 | clip 1
INFO: Epoch 054: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5
INFO: Epoch 055: loss 2.863 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.26 | clip 1
INFO: Epoch 055: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7
INFO: Epoch 056: loss 2.847 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.12 | clip 1
INFO: Epoch 056: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4
INFO: Epoch 057: loss 2.848 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.33 | clip 1
INFO: Epoch 057: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3
INFO: Epoch 058: loss 2.847 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.4 | clip 1
INFO: Epoch 058: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4
INFO: Epoch 059: loss 2.837 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.9 | clip 1
INFO: Epoch 059: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1
INFO: Epoch 060: loss 2.835 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.2 | clip 1
INFO: Epoch 060: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2
INFO: Epoch 061: loss 2.83 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.32 | clip 1
INFO: Epoch 061: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3
INFO: Epoch 062: loss 2.819 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.33 | clip 1
INFO: Epoch 062: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8
INFO: Epoch 063: loss 2.817 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.5 | clip 1
INFO: Epoch 063: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4
INFO: Epoch 064: loss 2.809 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.71 | clip 1
INFO: Epoch 064: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2
INFO: Epoch 065: loss 2.81 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.48 | clip 1
INFO: Epoch 065: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2
INFO: Epoch 066: loss 2.805 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.35 | clip 1
INFO: Epoch 066: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
INFO: Commencing training!
INFO: COMMAND: train.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --source-lang fr --target-lang en --save-dir /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7 --encoder-dropout-in 0.7 --encoder-dropout-out 0.7 --decoder-dropout-in 0.7 --decoder-dropout-out 0.7
INFO: Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.7, 'encoder_dropout_out': 0.7, 'decoder_dropout_in': 0.7, 'decoder_dropout_out': 0.7, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 5.058 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.78 | clip 1
INFO: Epoch 000: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169
INFO: Epoch 001: loss 4.67 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 53.2 | clip 1
INFO: Epoch 001: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 122
INFO: Epoch 002: loss 4.53 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 56.53 | clip 1
INFO: Epoch 002: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101
INFO: Epoch 003: loss 4.44 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 57.67 | clip 1
INFO: Epoch 003: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.7
INFO: Epoch 004: loss 4.364 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.4 | clip 1
INFO: Epoch 004: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.1
INFO: Epoch 005: loss 4.301 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.58 | clip 1
INFO: Epoch 005: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79
INFO: Epoch 006: loss 4.248 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.83 | clip 1
INFO: Epoch 006: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.9
INFO: Epoch 007: loss 4.199 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.7 | clip 1
INFO: Epoch 007: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.8
INFO: Epoch 008: loss 4.162 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 59.47 | clip 1
INFO: Epoch 008: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.7
INFO: Epoch 009: loss 4.129 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.97 | clip 1
INFO: Epoch 009: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.3
INFO: Epoch 010: loss 4.097 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 58.31 | clip 1
INFO: Epoch 010: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.2
INFO: Epoch 011: loss 4.076 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 57.83 | clip 1
INFO: Epoch 011: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59
INFO: Epoch 012: loss 4.048 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 56.86 | clip 1
INFO: Epoch 012: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.1
INFO: Epoch 013: loss 4.021 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 56.32 | clip 1
INFO: Epoch 013: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.8
INFO: Epoch 014: loss 4 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 55.85 | clip 1
INFO: Epoch 014: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.1
INFO: Epoch 015: loss 3.97 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 54.81 | clip 1
INFO: Epoch 015: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7
INFO: Epoch 016: loss 3.947 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 53.97 | clip 1
INFO: Epoch 016: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.5
INFO: Epoch 017: loss 3.932 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 53.22 | clip 1
INFO: Epoch 017: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.4
INFO: Epoch 018: loss 3.911 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 52.37 | clip 1
INFO: Epoch 018: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.5
INFO: Epoch 019: loss 3.89 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 51.39 | clip 1
INFO: Epoch 019: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51
INFO: Epoch 020: loss 3.875 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 50.54 | clip 1
INFO: Epoch 020: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.9
INFO: Epoch 021: loss 3.848 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 48.94 | clip 1
INFO: Epoch 021: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.1
INFO: Epoch 022: loss 3.827 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 47.76 | clip 1
INFO: Epoch 022: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.6
INFO: Epoch 023: loss 3.81 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 46.56 | clip 1
INFO: Epoch 023: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.9
INFO: Epoch 024: loss 3.8 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 45.03 | clip 1
INFO: Epoch 024: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.3
INFO: Epoch 025: loss 3.782 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 44.24 | clip 1
INFO: Epoch 025: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9
INFO: Epoch 026: loss 3.764 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 42.84 | clip 1
INFO: Epoch 026: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46
INFO: Epoch 027: loss 3.751 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 41.75 | clip 1
INFO: Epoch 027: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.3
INFO: Epoch 028: loss 3.741 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.77 | clip 1
INFO: Epoch 028: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.8
INFO: Epoch 029: loss 3.736 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 40.37 | clip 1
INFO: Epoch 029: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9
INFO: Epoch 030: loss 3.736 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.37 | clip 1
INFO: Epoch 030: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45
INFO: Epoch 031: loss 3.729 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.3 | clip 1
INFO: Epoch 031: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.7
INFO: Epoch 032: loss 3.724 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.15 | clip 1
INFO: Epoch 032: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8
INFO: Epoch 033: loss 3.712 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.16 | clip 1
INFO: Epoch 033: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.2
INFO: Epoch 034: loss 3.711 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 39.03 | clip 1
INFO: Epoch 034: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.1
INFO: Epoch 035: loss 3.702 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.62 | clip 1
INFO: Epoch 035: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.7
INFO: Epoch 036: loss 3.703 | lr 0.0003 | num_tokens 9.1 | batch_size 1 | grad_norm 38.7 | clip 1
INFO: Epoch 036: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
train done!
[2023-11-01 16:41:06] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.en.txt
[2023-11-01 16:41:06] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.0, 'encoder_dropout_out': 0.0, 'decoder_dropout_in': 0.0, 'decoder_dropout_out': 0.0, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.en.txt', 'max_len': 128}
[2023-11-01 16:41:06] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 16:41:06] Loaded a target dictionary (en) with 4000 words
[2023-11-01 16:41:06] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0/checkpoint_last.pt
[2023-11-01 17:04:23] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3.en.txt
[2023-11-01 17:04:23] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.3, 'encoder_dropout_out': 0.3, 'decoder_dropout_in': 0.3, 'decoder_dropout_out': 0.3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3.en.txt', 'max_len': 128}
[2023-11-01 17:04:23] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 17:04:23] Loaded a target dictionary (en) with 4000 words
[2023-11-01 17:04:23] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.3/checkpoint_last.pt
[2023-11-01 17:27:01] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5.en.txt
[2023-11-01 17:27:01] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.5, 'encoder_dropout_out': 0.5, 'decoder_dropout_in': 0.5, 'decoder_dropout_out': 0.5, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5.en.txt', 'max_len': 128}
[2023-11-01 17:27:01] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 17:27:01] Loaded a target dictionary (en) with 4000 words
[2023-11-01 17:27:01] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.5/checkpoint_last.pt
[2023-11-01 17:49:51] COMMAND: translate.py --data /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --dicts /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/ --checkpoint-path /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7/checkpoint_last.pt --output /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7.en.txt
[2023-11-01 17:49:51] Arguments: {'cuda': False, 'data': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_dropout_in': 0.7, 'encoder_dropout_out': 0.7, 'decoder_dropout_in': 0.7, 'decoder_dropout_out': 0.7, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//prepared/', 'checkpoint_path': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7/checkpoint_last.pt', 'output': '/cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7.en.txt', 'max_len': 128}
[2023-11-01 17:49:51] Loaded a source dictionary (fr) with 4000 words
[2023-11-01 17:49:51] Loaded a target dictionary (en) with 4000 words
[2023-11-01 17:49:51] Loaded a model from checkpoint /cluster/home/nanlin/atmt_2023/assignments/03/tune/../../../data/en-fr//tune--dropout/dropout_0.7/checkpoint_last.pt
translate done!
{
 "name": "BLEU",
 "score": 9.8,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "37.7/14.5/6.3/2.7 (BP = 1.000 ratio = 1.363 hyp_len = 5306 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
{
 "name": "BLEU",
 "score": 15.5,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "43.4/21.0/11.2/5.7 (BP = 1.000 ratio = 1.328 hyp_len = 5167 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
{
 "name": "BLEU",
 "score": 8.2,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "33.4/12.0/5.2/2.2 (BP = 1.000 ratio = 1.477 hyp_len = 5747 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
{
 "name": "BLEU",
 "score": 0.5,
 "signature": "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1",
 "verbose_score": "8.3/1.1/0.3/0.0 (BP = 1.000 ratio = 4.084 hyp_len = 15896 ref_len = 3892)",
 "nrefs": "1",
 "case": "mixed",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.3.1"
}
